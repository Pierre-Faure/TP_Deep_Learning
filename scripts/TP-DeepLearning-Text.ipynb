{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0IUo1pluqA53",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Deep learning - DE3 final Lab (Evaluation) \n",
    "\n",
    "This project belongs to the NLP domain. The task is straightforward: assign the correct job category to a job description. This is thus a multi-class classification task with 28 classes to choose from.\n",
    "The data has been retrieved from CommonCrawl. The latter has been famously used to train OpenAI's GPT-3 model. The data is therefore representative of what can be found on the English speaking part of the Internet. The goal of this project is to design a solution that accurate to predict the job based on the job descriptions. \n",
    "\n",
    "## Evaluation\n",
    "\n",
    "First of all, solutions are evaluated according to the Macro F1 metric, The Macro F1 score is simply the arithmetic average of the F1 score for each class.\n",
    "\n",
    "## Datasets\n",
    "\n",
    "**data.json**\n",
    "Contains job descriptions as well as genders for the training set, which contains 217,197 samples. If you're using pandas, then you can easily open this with pd.read_json.\n",
    "\n",
    "**label.csv**\n",
    "Contains job labels for the training set.\n",
    "\n",
    "**categories_string.csv**\n",
    "Provides a mapping between job labels and label integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1649314416781,
     "user": {
      "displayName": "Pierre Faure",
      "userId": "08701900717720912217"
     },
     "user_tz": -120
    },
    "id": "LeGHtvvTqA5-",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Import modules\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1435,
     "status": "ok",
     "timestamp": 1649314420284,
     "user": {
      "displayName": "Pierre Faure",
      "userId": "08701900717720912217"
     },
     "user_tz": -120
    },
    "id": "1JtVTXBwsdBm",
    "outputId": "86dc27df-994a-4002-ad93-83be6f9efcb7",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# montage gdrive pour colab\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 603,
     "status": "ok",
     "timestamp": 1649316161825,
     "user": {
      "displayName": "Pierre Faure",
      "userId": "08701900717720912217"
     },
     "user_tz": -120
    },
    "id": "-VkMt_qZ5i8X",
    "outputId": "822c29af-7f46-4079-9016-d30c8a8ea5d8",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# !cat /proc/cpuinfo\n",
    "# !cat /proc/meminfo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YFhLnNCZqA6A",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Baseline 1 (no deep learning)\n",
    "\n",
    "For this part we propose to train and test a straightforward logistic regression model on the tf-idf vectors extracted from the corpus. We are not applying any particular pre-processing to start and will progressively try to reduce the dimension of the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 1565,
     "status": "ok",
     "timestamp": 1649320451203,
     "user": {
      "displayName": "Pierre Faure",
      "userId": "08701900717720912217"
     },
     "user_tz": -120
    },
    "id": "hu-u_dCgqA6A",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Read all three files to obtain three dataframes (pandas)\n",
    "\n",
    "# read train.json\n",
    "df = pd.read_json(\"../data/train.json\")\n",
    "\n",
    "# read label\n",
    "label = pd.read_csv(\"../data/label.csv\")\n",
    "\n",
    "# read categories_string\n",
    "categories = pd.read_csv(\"../data/categories_string.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1649320451204,
     "user": {
      "displayName": "Pierre Faure",
      "userId": "08701900717720912217"
     },
     "user_tz": -120
    },
    "id": "0AIIHzn8qA6C",
    "outputId": "ee47ff8c-6149-487d-e478-a3a18f91a587",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Id                                        description gender\n",
      "0   0   She is also a Ronald D. Asmus Policy Entrepre...      F\n",
      "1   1   He is a member of the AICPA and WICPA. Brent ...      M\n",
      "2   2   Dr. Aster has held teaching and research posi...      M\n",
      "4   3   He runs a boutique design studio attending cl...      M\n",
      "5   4   He focuses on cloud security, identity and ac...      M\n",
      "7   5   He is author of several books, including the ...      M\n",
      "   Id  Category\n",
      "0   0        19\n",
      "1   1         9\n",
      "2   2        19\n",
      "3   3        24\n",
      "4   4        24\n",
      "5   5        22\n",
      "                  0  1\n",
      "0            pastor  0\n",
      "1             model  1\n",
      "2      yoga_teacher  2\n",
      "3           teacher  3\n",
      "4  personal_trainer  4\n",
      "5           painter  5\n"
     ]
    }
   ],
   "source": [
    "# Display the 3 first rows of each dataframe\n",
    "print(df.head(6))\n",
    "print(label.head(6))\n",
    "print(categories.head(6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1649320451204,
     "user": {
      "displayName": "Pierre Faure",
      "userId": "08701900717720912217"
     },
     "user_tz": -120
    },
    "id": "5lX4j4O0qA6C",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Add a column in df based on the column Category from the label dataframe\n",
    "#df['label'] = label['Category']\n",
    "df = pd.merge(df, label, on='Id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 483
    },
    "executionInfo": {
     "elapsed": 627,
     "status": "ok",
     "timestamp": 1649320451826,
     "user": {
      "displayName": "Pierre Faure",
      "userId": "08701900717720912217"
     },
     "user_tz": -120
    },
    "id": "tpx2quNMqA6D",
    "outputId": "1c76ee94-1b87-49ec-98cb-b9881ec0dec6",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x576 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize the distribution of the gender attribute and the Category one using appropriate graphics (barplot)\n",
    "# Example of visualisation libraries: seaborn, matplotlib etc. \n",
    "\n",
    "plt.figure(figsize=(16,8))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(df['gender'], bins=3);\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.hist(df['Category']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UAV4AHVDqA6E",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**What do you observe ?**\n",
    "\n",
    "Your answer: \n",
    "\n",
    "The distibution of gender is quite symetrical between male and female, but there are more men than women. For the category, there are some over-represented and some under-represented categories. This inequality can be problematic for the prediction step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_zDH5oFgqA6F",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "For the following part, let's focus on the top-5 jobs (category). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 260,
     "status": "ok",
     "timestamp": 1649320457073,
     "user": {
      "displayName": "Pierre Faure",
      "userId": "08701900717720912217"
     },
     "user_tz": -120
    },
    "id": "L9t50q04qA6G",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Create a new dataframe based on `df` but that contains observations that belongs to the top-5 most frequent jobs. \n",
    "top_5 = df['Category'].value_counts().index.values[:5]\n",
    "df_top5 = df[df['Category'].isin(top_5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 903,
     "status": "ok",
     "timestamp": 1649320487299,
     "user": {
      "displayName": "Pierre Faure",
      "userId": "08701900717720912217"
     },
     "user_tz": -120
    },
    "id": "BRL5INfRqA6H",
    "outputId": "5b46f8ee-5c36-4dbc-88b5-75aaf095f2a6",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pierr\\PycharmProjects\\TP_Deep_Learning\\venv\\lib\\site-packages\\pandas\\core\\indexing.py:1667: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.obj[key] = value\n"
     ]
    }
   ],
   "source": [
    "# Lets convert the text to lower case to reduce the size of the vocabulary\n",
    "df_top5.loc[:,\"description_lower\"] = [x.lower() for x in df_top5.description]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1649320488047,
     "user": {
      "displayName": "Pierre Faure",
      "userId": "08701900717720912217"
     },
     "user_tz": -120
    },
    "id": "sI3UkWVhqA6H",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Split the data into train and test \n",
    "X_train, X_test, y_train, y_test = train_test_split(df_top5[\"description_lower\"], \n",
    "                                                    df_top5[\"Category\"], test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 18055,
     "status": "ok",
     "timestamp": 1649320507495,
     "user": {
      "displayName": "Pierre Faure",
      "userId": "08701900717720912217"
     },
     "user_tz": -120
    },
    "id": "tIlDxlYgqA6H",
    "outputId": "297933bb-c1f6-4095-df7e-7a65e2e03c0d",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NB features: 121369\n"
     ]
    }
   ],
   "source": [
    "# Convert the text to tf-idf vectors \n",
    "transformer = TfidfVectorizer()\n",
    "transformer.fit(X_train.values)\n",
    "print(\"NB features: %d\" %(len(transformer.vocabulary_)))\n",
    "X_train = transformer.transform(X_train.values)\n",
    "X_test = transformer.transform(X_test.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1649320507495,
     "user": {
      "displayName": "Pierre Faure",
      "userId": "08701900717720912217"
     },
     "user_tz": -120
    },
    "id": "dfxxZYxQqA6I",
    "outputId": "79e95238-625b-4087-c3cc-044868efc5dc",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Taille de X_train: (86027, 121369)\n",
      "Taille de y_train: (86027,)\n"
     ]
    }
   ],
   "source": [
    "# Check the shape of X_train and y_train\n",
    "print(\"Taille de X_train: \" + str(X_train.shape))\n",
    "print(\"Taille de y_train: \" + str(y_train.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "executionInfo": {
     "elapsed": 84749,
     "status": "ok",
     "timestamp": 1649320592238,
     "user": {
      "displayName": "Pierre Faure",
      "userId": "08701900717720912217"
     },
     "user_tz": -120
    },
    "id": "OESNrNT3qA6I",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Fit a logistic regression model on X_train with 2000 max iterations\n",
    "model = LogisticRegression(random_state=42, max_iter=2000)\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 20,
     "status": "ok",
     "timestamp": 1649320592238,
     "user": {
      "displayName": "Pierre Faure",
      "userId": "08701900717720912217"
     },
     "user_tz": -120
    },
    "id": "GVryCaSfqA6J",
    "outputId": "3bbac184-bb82-4d00-e873-b784fd7e2849",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro F1:  0.897\n"
     ]
    }
   ],
   "source": [
    "# Check the macro f1-score\n",
    "test_f1 = f1_score(y_test, y_pred, average='macro')\n",
    "print('Macro F1:  %0.3f' % test_f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N55s26MmqA6J",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## What about pre-processing ? \n",
    "\n",
    "Let's try basic pre-processing for textual data. \n",
    "- removing stop words\n",
    "- lemmatization \n",
    "- stemmatization \n",
    "\n",
    "To proceed we can use the `nltk` library for instance (other alternatives : `sklearn`, `spacy`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 223,
     "status": "ok",
     "timestamp": 1649321436245,
     "user": {
      "displayName": "Pierre Faure",
      "userId": "08701900717720912217"
     },
     "user_tz": -120
    },
    "id": "Oz-Tpy1eqA6J",
    "outputId": "ec7c4c1e-4432-4366-ef0c-cbead8f02e54",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\pierr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Import nltk and the list of stop words\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "# Take a look at the list\n",
    "print(list(stop))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 20497,
     "status": "ok",
     "timestamp": 1649321457884,
     "user": {
      "displayName": "Pierre Faure",
      "userId": "08701900717720912217"
     },
     "user_tz": -120
    },
    "id": "Pv9CXrV5qA6K",
    "outputId": "eca4be69-10ed-4eed-f52b-faa3d64b0238",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pierr\\PycharmProjects\\TP_Deep_Learning\\venv\\lib\\site-packages\\ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "# Remove stop words from df \n",
    "df_top5['description_wo_stop'] = df_top5[\"description_lower\"].apply(\n",
    "    lambda words: ' '.join(word.lower() for word in words.split() if word not in stop))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "executionInfo": {
     "elapsed": 233,
     "status": "ok",
     "timestamp": 1649321469473,
     "user": {
      "displayName": "Pierre Faure",
      "userId": "08701900717720912217"
     },
     "user_tz": -120
    },
    "id": "R404juu2qA6L",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df_top5[\"description_wo_stop\"], \n",
    "                                                    df_top5[\"Category\"], test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10935,
     "status": "ok",
     "timestamp": 1649321482025,
     "user": {
      "displayName": "Pierre Faure",
      "userId": "08701900717720912217"
     },
     "user_tz": -120
    },
    "id": "vEfpbovvqA6M",
    "outputId": "b18a4e5f-b46a-4094-e812-d75f24d6413d",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NB features: 121237\n"
     ]
    }
   ],
   "source": [
    "# Tf-idf vectorization\n",
    "transformer = TfidfVectorizer(stop_words=list(stop)).fit(X_train.values)\n",
    "print(\"NB features: %d\" %(len(transformer.vocabulary_)))\n",
    "X_train = transformer.transform(X_train.values)\n",
    "X_test = transformer.transform(X_test.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 80320,
     "status": "ok",
     "timestamp": 1649321564446,
     "user": {
      "displayName": "Pierre Faure",
      "userId": "08701900717720912217"
     },
     "user_tz": -120
    },
    "id": "uoZsfOkxqA6M",
    "outputId": "1928c77a-8019-4d4e-f2b4-d781ed31e388",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro F1:  0.897\n"
     ]
    }
   ],
   "source": [
    "# Logistic regression - Same as before\n",
    "model = LogisticRegression(random_state=42, max_iter=2000)\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "test_f1 = f1_score(y_test, y_pred, average='macro')\n",
    "print('Macro F1:  %0.3f' % test_f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EuUbLQ45qA6M",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**What do you observe ?**\n",
    "\n",
    "Your answer : First we see that the number of words in the vocabulary is less important, as we've removed the stopwords. \n",
    "But the impact of removing stopwords on the result is really small."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1550,
     "status": "ok",
     "timestamp": 1649321699677,
     "user": {
      "displayName": "Pierre Faure",
      "userId": "08701900717720912217"
     },
     "user_tz": -120
    },
    "id": "Wn-7JiJYqA6N",
    "outputId": "c79293fd-c661-4480-c552-751ae1269355",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\pierr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\pierr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Let add some more pre-processing. \n",
    "\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import WordNetLemmatizer,PorterStemmer\n",
    "import re\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = PorterStemmer() \n",
    "\n",
    "def preprocess(sentence):\n",
    "    sentence=str(sentence)\n",
    "    # Lowercase text\n",
    "    sentence = sentence.lower()\n",
    "    # Remove whitespace\n",
    "    sentence=sentence.replace('{html}',\"\") \n",
    "    cleanr = re.compile('<.*?>')\n",
    "    cleantext = re.sub(cleanr, '', sentence)\n",
    "    # Remove weblinks\n",
    "    rem_url=re.sub(r'http\\S+', '',cleantext)\n",
    "    # Remove numbers\n",
    "    rem_num = re.sub('[0-9]+', '', rem_url)\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    tokens = tokenizer.tokenize(rem_num)  \n",
    "    filtered_words = [w for w in tokens if len(w) > 2 if not w in stopwords.words('english')]\n",
    "    #stem_words=[stemmer.stem(w) for w in filtered_words]\n",
    "    #lemma_words=[lemmatizer.lemmatize(w) for w in stem_words]\n",
    "    return \" \".join(filtered_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p-K_kzX5qA6N",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**Describe the different pre-processing methods used by the `preprocess` function**\n",
    "\n",
    "Your answer: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "z_D8m504qA6N",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp\\ipykernel_16532\\3399844408.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[1;31m# Let's apply the function to our data\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 2\u001B[1;33m \u001B[0mdf_top5\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;34m'description_pre'\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mdf_top5\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;34m'description'\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mmap\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;32mlambda\u001B[0m \u001B[0ms\u001B[0m\u001B[1;33m:\u001B[0m\u001B[0mpreprocess\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0ms\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[1;32m~\\PycharmProjects\\TP_Deep_Learning\\venv\\lib\\site-packages\\pandas\\core\\series.py\u001B[0m in \u001B[0;36mmap\u001B[1;34m(self, arg, na_action)\u001B[0m\n\u001B[0;32m   4159\u001B[0m         \u001B[0mdtype\u001B[0m\u001B[1;33m:\u001B[0m \u001B[0mobject\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   4160\u001B[0m         \"\"\"\n\u001B[1;32m-> 4161\u001B[1;33m         \u001B[0mnew_values\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0msuper\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_map_values\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0marg\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mna_action\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mna_action\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   4162\u001B[0m         return self._constructor(new_values, index=self.index).__finalize__(\n\u001B[0;32m   4163\u001B[0m             \u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mmethod\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;34m\"map\"\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\PycharmProjects\\TP_Deep_Learning\\venv\\lib\\site-packages\\pandas\\core\\base.py\u001B[0m in \u001B[0;36m_map_values\u001B[1;34m(self, mapper, na_action)\u001B[0m\n\u001B[0;32m    868\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    869\u001B[0m         \u001B[1;31m# mapper is a function\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 870\u001B[1;33m         \u001B[0mnew_values\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mmap_f\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mvalues\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mmapper\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    871\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    872\u001B[0m         \u001B[1;32mreturn\u001B[0m \u001B[0mnew_values\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\PycharmProjects\\TP_Deep_Learning\\venv\\lib\\site-packages\\pandas\\_libs\\lib.pyx\u001B[0m in \u001B[0;36mpandas._libs.lib.map_infer\u001B[1;34m()\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Local\\Temp\\ipykernel_16532\\3399844408.py\u001B[0m in \u001B[0;36m<lambda>\u001B[1;34m(s)\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[1;31m# Let's apply the function to our data\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 2\u001B[1;33m \u001B[0mdf_top5\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;34m'description_pre'\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mdf_top5\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;34m'description'\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mmap\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;32mlambda\u001B[0m \u001B[0ms\u001B[0m\u001B[1;33m:\u001B[0m\u001B[0mpreprocess\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0ms\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[1;32m~\\AppData\\Local\\Temp\\ipykernel_16532\\3909632233.py\u001B[0m in \u001B[0;36mpreprocess\u001B[1;34m(sentence)\u001B[0m\n\u001B[0;32m     25\u001B[0m     \u001B[0mtokenizer\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mRegexpTokenizer\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34mr'\\w+'\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     26\u001B[0m     \u001B[0mtokens\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mtokenizer\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtokenize\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mrem_num\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 27\u001B[1;33m     \u001B[0mfiltered_words\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m[\u001B[0m\u001B[0mw\u001B[0m \u001B[1;32mfor\u001B[0m \u001B[0mw\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mtokens\u001B[0m \u001B[1;32mif\u001B[0m \u001B[0mlen\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mw\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;33m>\u001B[0m \u001B[1;36m2\u001B[0m \u001B[1;32mif\u001B[0m \u001B[1;32mnot\u001B[0m \u001B[0mw\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mstopwords\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mwords\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m'english'\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     28\u001B[0m     \u001B[1;31m#stem_words=[stemmer.stem(w) for w in filtered_words]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     29\u001B[0m     \u001B[1;31m#lemma_words=[lemmatizer.lemmatize(w) for w in stem_words]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Local\\Temp\\ipykernel_16532\\3909632233.py\u001B[0m in \u001B[0;36m<listcomp>\u001B[1;34m(.0)\u001B[0m\n\u001B[0;32m     25\u001B[0m     \u001B[0mtokenizer\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mRegexpTokenizer\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34mr'\\w+'\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     26\u001B[0m     \u001B[0mtokens\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mtokenizer\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtokenize\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mrem_num\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 27\u001B[1;33m     \u001B[0mfiltered_words\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m[\u001B[0m\u001B[0mw\u001B[0m \u001B[1;32mfor\u001B[0m \u001B[0mw\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mtokens\u001B[0m \u001B[1;32mif\u001B[0m \u001B[0mlen\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mw\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;33m>\u001B[0m \u001B[1;36m2\u001B[0m \u001B[1;32mif\u001B[0m \u001B[1;32mnot\u001B[0m \u001B[0mw\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mstopwords\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mwords\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m'english'\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     28\u001B[0m     \u001B[1;31m#stem_words=[stemmer.stem(w) for w in filtered_words]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     29\u001B[0m     \u001B[1;31m#lemma_words=[lemmatizer.lemmatize(w) for w in stem_words]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\PycharmProjects\\TP_Deep_Learning\\venv\\lib\\site-packages\\nltk\\corpus\\reader\\wordlist.py\u001B[0m in \u001B[0;36mwords\u001B[1;34m(self, fileids, ignore_lines_startswith)\u001B[0m\n\u001B[0;32m     19\u001B[0m         return [\n\u001B[0;32m     20\u001B[0m             \u001B[0mline\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 21\u001B[1;33m             \u001B[1;32mfor\u001B[0m \u001B[0mline\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mline_tokenize\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mraw\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mfileids\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     22\u001B[0m             \u001B[1;32mif\u001B[0m \u001B[1;32mnot\u001B[0m \u001B[0mline\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mstartswith\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mignore_lines_startswith\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     23\u001B[0m         ]\n",
      "\u001B[1;32m~\\PycharmProjects\\TP_Deep_Learning\\venv\\lib\\site-packages\\nltk\\corpus\\reader\\api.py\u001B[0m in \u001B[0;36mraw\u001B[1;34m(self, fileids)\u001B[0m\n\u001B[0;32m    216\u001B[0m         \u001B[0mcontents\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m[\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    217\u001B[0m         \u001B[1;32mfor\u001B[0m \u001B[0mf\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mfileids\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 218\u001B[1;33m             \u001B[1;32mwith\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mopen\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mf\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0mfp\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    219\u001B[0m                 \u001B[0mcontents\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mappend\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mfp\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mread\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    220\u001B[0m         \u001B[1;32mreturn\u001B[0m \u001B[0mconcat\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mcontents\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\PycharmProjects\\TP_Deep_Learning\\venv\\lib\\site-packages\\nltk\\corpus\\reader\\api.py\u001B[0m in \u001B[0;36mopen\u001B[1;34m(self, file)\u001B[0m\n\u001B[0;32m    229\u001B[0m         \"\"\"\n\u001B[0;32m    230\u001B[0m         \u001B[0mencoding\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mencoding\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mfile\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 231\u001B[1;33m         \u001B[0mstream\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_root\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mjoin\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mfile\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mopen\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mencoding\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    232\u001B[0m         \u001B[1;32mreturn\u001B[0m \u001B[0mstream\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    233\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\PycharmProjects\\TP_Deep_Learning\\venv\\lib\\site-packages\\nltk\\data.py\u001B[0m in \u001B[0;36mjoin\u001B[1;34m(self, fileid)\u001B[0m\n\u001B[0;32m    332\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0mjoin\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mfileid\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    333\u001B[0m         \u001B[0m_path\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mos\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpath\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mjoin\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_path\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mfileid\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 334\u001B[1;33m         \u001B[1;32mreturn\u001B[0m \u001B[0mFileSystemPathPointer\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0m_path\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    335\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    336\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0m__repr__\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\PycharmProjects\\TP_Deep_Learning\\venv\\lib\\site-packages\\nltk\\compat.py\u001B[0m in \u001B[0;36m_decorator\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m     39\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0m_decorator\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     40\u001B[0m         \u001B[0margs\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m(\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m0\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0madd_py3_data\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;33m+\u001B[0m \u001B[0margs\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m2\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 41\u001B[1;33m         \u001B[1;32mreturn\u001B[0m \u001B[0minit_func\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     42\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     43\u001B[0m     \u001B[1;32mreturn\u001B[0m \u001B[0mwraps\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0minit_func\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0m_decorator\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\PycharmProjects\\TP_Deep_Learning\\venv\\lib\\site-packages\\nltk\\data.py\u001B[0m in \u001B[0;36m__init__\u001B[1;34m(self, _path)\u001B[0m\n\u001B[0;32m    309\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    310\u001B[0m         \u001B[0m_path\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mos\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpath\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mabspath\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0m_path\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 311\u001B[1;33m         \u001B[1;32mif\u001B[0m \u001B[1;32mnot\u001B[0m \u001B[0mos\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpath\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mexists\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0m_path\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    312\u001B[0m             \u001B[1;32mraise\u001B[0m \u001B[0mOSError\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m\"No such file or directory: %r\"\u001B[0m \u001B[1;33m%\u001B[0m \u001B[0m_path\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    313\u001B[0m         \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_path\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0m_path\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\Anaconda3\\lib\\genericpath.py\u001B[0m in \u001B[0;36mexists\u001B[1;34m(path)\u001B[0m\n\u001B[0;32m     17\u001B[0m     \u001B[1;34m\"\"\"Test whether a path exists.  Returns False for broken symbolic links\"\"\"\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     18\u001B[0m     \u001B[1;32mtry\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 19\u001B[1;33m         \u001B[0mos\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mstat\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mpath\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     20\u001B[0m     \u001B[1;32mexcept\u001B[0m \u001B[0mOSError\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     21\u001B[0m         \u001B[1;32mreturn\u001B[0m \u001B[1;32mFalse\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "# Let's apply the function to our data\n",
    "df_top5['description_pre']=df_top5['description'].map(lambda s:preprocess(s)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8Jv3UBLbqA6O",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Following the same steps as before, split the data into a train and a test set. They learn a logistic regression and print the f1 score obtained. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "4nNWlG6XqA6O",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'description_pre'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "\u001B[1;32m~\\PycharmProjects\\TP_Deep_Learning\\venv\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001B[0m in \u001B[0;36mget_loc\u001B[1;34m(self, key, method, tolerance)\u001B[0m\n\u001B[0;32m   3360\u001B[0m             \u001B[1;32mtry\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 3361\u001B[1;33m                 \u001B[1;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_engine\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mget_loc\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mcasted_key\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   3362\u001B[0m             \u001B[1;32mexcept\u001B[0m \u001B[0mKeyError\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0merr\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\PycharmProjects\\TP_Deep_Learning\\venv\\lib\\site-packages\\pandas\\_libs\\index.pyx\u001B[0m in \u001B[0;36mpandas._libs.index.IndexEngine.get_loc\u001B[1;34m()\u001B[0m\n",
      "\u001B[1;32m~\\PycharmProjects\\TP_Deep_Learning\\venv\\lib\\site-packages\\pandas\\_libs\\index.pyx\u001B[0m in \u001B[0;36mpandas._libs.index.IndexEngine.get_loc\u001B[1;34m()\u001B[0m\n",
      "\u001B[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001B[0m in \u001B[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001B[1;34m()\u001B[0m\n",
      "\u001B[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001B[0m in \u001B[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001B[1;34m()\u001B[0m\n",
      "\u001B[1;31mKeyError\u001B[0m: 'description_pre'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001B[1;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp\\ipykernel_16532\\4207089172.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[1;31m# Your answer\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 2\u001B[1;33m \u001B[0mdf_top5\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;34m'description_pre'\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[1;32m~\\PycharmProjects\\TP_Deep_Learning\\venv\\lib\\site-packages\\pandas\\core\\frame.py\u001B[0m in \u001B[0;36m__getitem__\u001B[1;34m(self, key)\u001B[0m\n\u001B[0;32m   3456\u001B[0m             \u001B[1;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mcolumns\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mnlevels\u001B[0m \u001B[1;33m>\u001B[0m \u001B[1;36m1\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   3457\u001B[0m                 \u001B[1;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_getitem_multilevel\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mkey\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 3458\u001B[1;33m             \u001B[0mindexer\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mcolumns\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mget_loc\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mkey\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   3459\u001B[0m             \u001B[1;32mif\u001B[0m \u001B[0mis_integer\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mindexer\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   3460\u001B[0m                 \u001B[0mindexer\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m[\u001B[0m\u001B[0mindexer\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\PycharmProjects\\TP_Deep_Learning\\venv\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001B[0m in \u001B[0;36mget_loc\u001B[1;34m(self, key, method, tolerance)\u001B[0m\n\u001B[0;32m   3361\u001B[0m                 \u001B[1;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_engine\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mget_loc\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mcasted_key\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   3362\u001B[0m             \u001B[1;32mexcept\u001B[0m \u001B[0mKeyError\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0merr\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 3363\u001B[1;33m                 \u001B[1;32mraise\u001B[0m \u001B[0mKeyError\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mkey\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0merr\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   3364\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   3365\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[0mis_scalar\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mkey\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;32mand\u001B[0m \u001B[0misna\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mkey\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;32mand\u001B[0m \u001B[1;32mnot\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mhasnans\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mKeyError\u001B[0m: 'description_pre'"
     ]
    }
   ],
   "source": [
    "# Your answer\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_top5[\"description_pre\"], \n",
    "                                                    df_top5[\"Category\"], test_size=0.33, random_state=42)\n",
    "\n",
    "model = LogisticRegression(random_state=42, max_iter=2000)\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "test_f1 = f1_score(y_test, y_pred, average='macro')\n",
    "print('Macro F1:  %0.3f' % test_f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ycSFZeroqA6O",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**What do you observe ?**\n",
    "\n",
    "Your answer: Even if the data is \"cleaner\" due to the eavier preprocessing, we dont observe better performance. The fit/predict might be less sloow but the prepocessing step is very long so this is not worth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z0KQCwzAqA6O",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Let's try to tune the regularisation parameter of our model. \n",
    "\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Initialize the model\n",
    "...\n",
    "\n",
    "# Define the grid search\n",
    "...\n",
    "\n",
    "# Summarize results\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "foQSNehKqA6P",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Let's dive into Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JCyk9MLoqA6P",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Transformers and attention mechanisms\n",
    "The paper Attention Is All You Need describes transformers and what is called a \n",
    "sequence-to-sequence architecture. Sequence-to-Sequence (or Seq2Seq) is a neural net that transforms\n",
    "a given sequence of elements, such as the sequence of words in a sentence, into another sequence. \n",
    "\n",
    "Seq2Seq models are particularly good at translation, where the sequence of words from one language is transformed into a sequence of different words in another language. A popular choice for this type of model is Long-Short-Term-Memory (LSTM)-based models. With sequence-dependent data, the LSTM modules can give meaning to the sequence while remembering (or forgetting) the parts it finds important (or unimportant). Sentences, for example, are sequence-dependent since the order of the words is crucial for understanding the sentence. LSTM are a natural choice for this type of data.\n",
    "\n",
    "Seq2Seq models consist of an Encoder and a Decoder. The Encoder takes the input sequence and maps it into a higher dimensional space (n-dimensional vector). That abstract vector is fed into the Decoder which turns it into an output sequence. The output sequence can be in another language, symbols, a copy of the input, etc. To solve the task of document classification we will use the pre-trained model **Bert**. \n",
    "\n",
    "#### TO DO \n",
    "\n",
    "1. As a first exercice, you will have to read the paper **Attention Is All You Need**, to get a general understanding of transformers. You can also look for additional resources online (tutorial, video etc.) To keep things clear I recommend to focus on transformers for NLP (text). The paper can be found [here](https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf). \n",
    "\n",
    "**This step is mandatory as you will need to unserstand the basics of transformers for what will follow. In addition, you will have questions specifically related to this architecture during your last evaluation.**\n",
    "\n",
    "2. We will then use a pre-trained model based usin the popular [Huggin Face](https://huggingface.co/docs/transformers/index) for pytorch. More precisely, we plan on using the [Bert](https://huggingface.co/docs/transformers/model_doc/bert) model. The following cells guide you toward using Bert. On top of applying the model to our data, you will have to answer a few questions along the project (including the ones above). These **questions are mandatory**. \n",
    "\n",
    "3. Finally, the last step is the fine-tuning. Generally speaking, fine tuning a model refers to re-training the last layers of a deep architecture on your data. Again, you will be guided on how to proceed. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y-tEKnr3qA6Q",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**Lets start !**\n",
    "The first thing that you need to do is to install the transformers librariy \n",
    "\n",
    "`import sys\n",
    "!{sys.executable} -m pip install transformers`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "ogeqZm3lqA6Q",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'transformers'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp\\ipykernel_16532\\807322805.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[1;31m# Import the library\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 2\u001B[1;33m \u001B[1;32mfrom\u001B[0m \u001B[0mtransformers\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mAutoModelWithLMHead\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mAutoTokenizer\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      3\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mtorch\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      4\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0mtransformers\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mBertTokenizer\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mModuleNotFoundError\u001B[0m: No module named 'transformers'"
     ]
    }
   ],
   "source": [
    "# Import the library\n",
    "from transformers import AutoModelWithLMHead, AutoTokenizer\n",
    "import torch \n",
    "from transformers import BertTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FlzdKM5gqA6Q",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Let's take a look at some popular pre-trained models available in Huggingface. \n",
    "\n",
    "1. We start with the model Camembert [Camembert](https://camembert-model.fr/) trained on french corpus by Facebook and Inria teams. \n",
    "2. GPT2 a pre-trained model for text generation\n",
    "3. Bert !\n",
    "\n",
    "**Note:** You can also directly try the online [demo](https://transformer.huggingface.co/) of the library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q0lv81Q5qA6R",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Example 1 - Camembert\n",
    "\n",
    "model_name = \"camembert-base\"# try also distilbert-base-cased for english\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelWithLMHead.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6f1RavSOqA6R",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "all_sequences = [f\"Jacques Chirac est un {tokenizer.mask_token}\", \n",
    "            f\"Antoine Griezman est un {tokenizer.mask_token}\",\n",
    "            f\"Le camembert, c'est {tokenizer.mask_token}\"]\n",
    "\n",
    "for sequence in all_sequences:\n",
    "    input = tokenizer.encode(sequence, return_tensors=\"pt\")\n",
    "    mask_token_index = torch.where(input == tokenizer.mask_token_id)[1]\n",
    "    token_logits = model(input).logits\n",
    "    mask_token_logits = token_logits[0, mask_token_index, :]\n",
    "    top_5_tokens = torch.topk(mask_token_logits, 5, dim=1).indices[0].tolist()\n",
    "    for token in top_5_tokens:\n",
    "        print(sequence.replace(tokenizer.mask_token, tokenizer.decode([token])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qbbzcKy-qA6R",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Example 2 - GPT2 for text generation \n",
    "\n",
    "# Your turn : find a small example on how to use GPT2 to generate text with pytorch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "10zVGFeVqA6S",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Finally, Bert !! Let's see next time :) "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "TP-DeepLearning-Text.ipynb",
   "version": ""
  },
  "kernelspec": {
   "display_name": "TP_Deep_Learning_env",
   "language": "python",
   "name": "tp_deep_learning_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}